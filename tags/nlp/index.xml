<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on hongyuntw</title>
    <link>/tags/nlp/</link>
    <description>Recent content in NLP on hongyuntw</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 Oct 2021 06:00:20 +0600</lastBuildDate><atom:link href="/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MacBERT：MLM as correction BERT</title>
      <link>/posts/macbert/</link>
      <pubDate>Sat, 23 Oct 2021 06:00:20 +0600</pubDate>
      
      <guid>/posts/macbert/</guid>
      <description>簡介 這顆Model是朋友跟我說可以用用看，很多中文的比賽都用這個Model我才知道，因此簡單記錄一下 這個Model 主要有這些貢獻 進行了廣泛的廣播研究，</description>
    </item>
    
    <item>
      <title>RocketQA : An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering</title>
      <link>/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</link>
      <pubDate>Fri, 20 Aug 2021 06:00:20 +0600</pubDate>
      
      <guid>/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</guid>
      <description>Abstract + Introduction difficult to effectively train a dual-encoder for dense passage retrieval retriever needs to identify positive passages for each question from a large collection there might be a large number of unlabeled positives (IR資料集可能有錯誤) it is expensive to acquire large-scale training data for open-domain QA 反正就是large scale 的open-d</description>
    </item>
    
    <item>
      <title>A Discriminative Semantic Ranker for Question Retrieval</title>
      <link>/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</link>
      <pubDate>Sun, 17 Jan 2021 08:06:25 +0600</pubDate>
      
      <guid>/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</guid>
      <description>Abstract + Introduction First-stage ranker 的新架構 DenseTrans - Transformer + DenseNet 的融合物 主要的任務是question retrieval，找出與使用者輸入相似的問題，類似FAQ Term-based的方式會</description>
    </item>
    
  </channel>
</rss>
