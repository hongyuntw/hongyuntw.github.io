<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on hyc&#39;s blog</title>
    <link>https://hongyuntw.github.io/posts/</link>
    <description>Recent content in Posts on hyc&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 05 Feb 2025 10:36:26 +0800</lastBuildDate>
    <atom:link href="https://hongyuntw.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>工作一年心得</title>
      <link>https://hongyuntw.github.io/posts/one-year-work/</link>
      <pubDate>Wed, 05 Feb 2025 10:36:26 +0800</pubDate>
      <guid>https://hongyuntw.github.io/posts/one-year-work/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;騎車過程想到，可以透過文字紀錄一下自己剛開始工作的心得，未來自己 or 小孩看到應該是會覺得很有趣吧，也怕過了太久自己忘記了，所以才想記錄一下~~&lt;/p&gt;&#xA;&lt;h2 id=&#34;選擇成為軟體工程師&#34;&gt;選擇成為軟體工程師&lt;/h2&gt;&#xA;&lt;h2 id=&#34;一年來學到的東西&#34;&gt;一年來學到的東西&lt;/h2&gt;&#xA;&lt;h2 id=&#34;未來規劃&#34;&gt;未來規劃&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Aicup2021 農業文章文字標註及辨識 比賽心得與報告</title>
      <link>https://hongyuntw.github.io/posts/aicup2021/</link>
      <pubDate>Fri, 17 Dec 2021 13:55:59 +0800</pubDate>
      <guid>https://hongyuntw.github.io/posts/aicup2021/</guid>
      <description>&lt;h2 id=&#34;基本資訊&#34;&gt;基本資訊&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;隊伍名稱 : Rule-Based Method&lt;/li&gt;&#xA;&lt;li&gt;最終成績 : 7th&#xA;&lt;ul&gt;&#xA;&lt;li&gt;0.8063660 7/118 precision : 0.82608 recall : 0.78756&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ga8Rkwb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;先針對主辦單位所提供的excel檔，創造出一個替換的dictionary&#xA;&lt;ul&gt;&#xA;&lt;li&gt;比如說 &lt;img src=&#34;https://i.imgur.com/L6MESeQ.png&#34; alt=&#34;&#34;&gt;，就把斜紋夜盜、黑蟲、行軍蟲&amp;hellip;全部換成斜紋夜蛾，方面後續model使用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;對每篇文章利用ckiptagger做斷詞&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在斷詞的時候要使用 &lt;code&gt;coerce_dictionary&lt;/code&gt; 這個參數，這個參數可以讓主辦單位提供的keywords，被強制斷出來，不然可能會發生 &amp;ldquo;黑蟲&amp;rdquo; 被斷成 [&amp;ldquo;黑&amp;rdquo; &amp;ldquo;蟲&amp;rdquo;]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;把斷好詞的結果做keywords的替換&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ex: 把所有 &amp;ldquo;黑蟲&amp;rdquo; 換成 &amp;ldquo;斜紋夜盜&amp;rdquo; 這樣&lt;/li&gt;&#xA;&lt;li&gt;在這一步的時候要注意一些特別case，避免被重複替換到&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;針對每篇文章，取出該篇文章所有出現的keyword集合&#xA;&lt;ul&gt;&#xA;&lt;li&gt;比如說 1.txt這篇文章 keyword_set = [&amp;ldquo;黑點病&amp;rdquo;,&amp;ldquo;降雨&amp;rdquo;,&amp;ldquo;文旦柚&amp;rdquo;,&amp;ldquo;鋅錳乃浦&amp;rdquo;]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;&#xA;&lt;p&gt;在這邊我們一開始的思路有幾種&lt;/p&gt;</description>
    </item>
    <item>
      <title>MacBERT：MLM as correction BERT</title>
      <link>https://hongyuntw.github.io/posts/macbert/</link>
      <pubDate>Sat, 23 Oct 2021 06:00:20 +0600</pubDate>
      <guid>https://hongyuntw.github.io/posts/macbert/</guid>
      <description>&lt;h2 id=&#34;簡介&#34;&gt;簡介&lt;/h2&gt;&#xA;&lt;p&gt;這顆Model是朋友跟我說可以用用看，很多中文的比賽都用這個Model我才知道，因此簡單記錄一下&lt;/p&gt;&#xA;&lt;p&gt;這個Model 主要有這些貢獻&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;進行了廣泛的廣播研究，通過詳細分析語言中文預訓練語言模型在各種任務上的表達。&lt;/li&gt;&#xA;&lt;li&gt;改善傳統BERT欲訓練[MASK] token 不出現在下游任務，導致預訓練任務跟實際任務不符的缺點，macBERT是用同義詞來取代token&lt;/li&gt;&#xA;&lt;li&gt;為了改善中文NLP，發布預訓練Model&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用 WWM (whole word masking) 跟 n-gram 來選要被替換的tokens&#xA;&lt;ul&gt;&#xA;&lt;li&gt;uni-gram - 4-gram 的比例是 40%、30%、20%、10%。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;原始BERT模型使用[MASK] token 進行mask，但是[MASK] token 在fine-tuning / downstream task不會出現，會造成預訓練任務與fintuning的不一致，因此用同義詞來替換token&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A similar word is obtained by using Synonyms toolkit (Wang and Hu, 2017), which is based on word2vec&lt;/li&gt;&#xA;&lt;li&gt;If an N-gram is selected to mask, we will find similar words individually. In rare cases, when there is no similar word, we will degrade to use random word replacement&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;15% input words for masking&#xA;&lt;ul&gt;&#xA;&lt;li&gt;80% will replace with similar words&lt;/li&gt;&#xA;&lt;li&gt;10% replace with a random word&lt;/li&gt;&#xA;&lt;li&gt;keep with original words for the rest of 10%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://hongyuntw.github.io/images/macBERT/1.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RocketQA : An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering</title>
      <link>https://hongyuntw.github.io/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</link>
      <pubDate>Fri, 20 Aug 2021 06:00:20 +0600</pubDate>
      <guid>https://hongyuntw.github.io/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</guid>
      <description>&lt;h2 id=&#34;abstract--introduction&#34;&gt;Abstract + Introduction&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;difficult to effectively train a dual-encoder for dense passage retrieval&#xA;&lt;ul&gt;&#xA;&lt;li&gt;retriever needs to identify positive passages for each question from a large collection&lt;/li&gt;&#xA;&lt;li&gt;there might be a large number of unlabeled positives (IR資料集可能有錯誤)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;it is expensive to acquire large-scale training data for open-domain QA&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;反正就是large scale 的open-domain QA Model很難train，所以提出了三種方法來改善整個流程&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Discriminative Semantic Ranker for Question Retrieval</title>
      <link>https://hongyuntw.github.io/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</link>
      <pubDate>Sun, 17 Jan 2021 08:06:25 +0600</pubDate>
      <guid>https://hongyuntw.github.io/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</guid>
      <description>&lt;h2 id=&#34;abstract--introduction&#34;&gt;Abstract + Introduction&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;First-stage ranker 的新架構 DenseTrans - Transformer + DenseNet 的融合物&lt;/li&gt;&#xA;&lt;li&gt;主要的任務是question retrieval，找出與使用者輸入相似的問題，類似FAQ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Term-based的方式會有 無法滿足 &lt;strong&gt;Semantic Requirement&lt;/strong&gt; 的缺點&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
