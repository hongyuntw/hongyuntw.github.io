<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on hyc&#39;s blog</title>
    <link>https://hongyuntw.github.io/categories/nlp/</link>
    <description>Recent content in NLP on hyc&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 17 Dec 2021 13:55:59 +0800</lastBuildDate><atom:link href="https://hongyuntw.github.io/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Aicup2021 農業文章文字標註及辨識 比賽心得與報告</title>
      <link>https://hongyuntw.github.io/posts/aicup2021/</link>
      <pubDate>Fri, 17 Dec 2021 13:55:59 +0800</pubDate>
      
      <guid>https://hongyuntw.github.io/posts/aicup2021/</guid>
      <description>基本資訊 隊伍名稱 : Rule-Based Method 最終成績 : 7th 0.8063660 7/118 precision : 0.82608 recall : 0.78756 Preprocessing 先針對主辦單位所提供的excel檔，創造出一個替換的dictionary 比如說 ，就把斜紋夜盜、黑蟲</description>
    </item>
    
    <item>
      <title>MacBERT：MLM as correction BERT</title>
      <link>https://hongyuntw.github.io/posts/macbert/</link>
      <pubDate>Sat, 23 Oct 2021 06:00:20 +0600</pubDate>
      
      <guid>https://hongyuntw.github.io/posts/macbert/</guid>
      <description>簡介 這顆Model是朋友跟我說可以用用看，很多中文的比賽都用這個Model我才知道，因此簡單記錄一下 這個Model 主要有這些貢獻 進行了廣泛的廣播研究，</description>
    </item>
    
    <item>
      <title>RocketQA : An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering</title>
      <link>https://hongyuntw.github.io/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</link>
      <pubDate>Fri, 20 Aug 2021 06:00:20 +0600</pubDate>
      
      <guid>https://hongyuntw.github.io/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</guid>
      <description>Abstract + Introduction difficult to effectively train a dual-encoder for dense passage retrieval retriever needs to identify positive passages for each question from a large collection there might be a large number of unlabeled positives (IR資料集可能有錯誤) it is expensive to acquire large-scale training data for open-domain QA 反正就是large scale 的open-d</description>
    </item>
    
    <item>
      <title>A Discriminative Semantic Ranker for Question Retrieval</title>
      <link>https://hongyuntw.github.io/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</link>
      <pubDate>Sun, 17 Jan 2021 08:06:25 +0600</pubDate>
      
      <guid>https://hongyuntw.github.io/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</guid>
      <description>Abstract + Introduction First-stage ranker 的新架構 DenseTrans - Transformer + DenseNet 的融合物 主要的任務是question retrieval，找出與使用者輸入相似的問題，類似FAQ Term-based的方式會</description>
    </item>
    
  </channel>
</rss>
