<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Papers on hyc&#39;s blog</title>
    <link>https://hongyuntw.github.io/categories/papers/</link>
    <description>Recent content in Papers on hyc&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 Oct 2021 06:00:20 +0600</lastBuildDate>
    <atom:link href="https://hongyuntw.github.io/categories/papers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MacBERT：MLM as correction BERT</title>
      <link>https://hongyuntw.github.io/posts/macbert/</link>
      <pubDate>Sat, 23 Oct 2021 06:00:20 +0600</pubDate>
      <guid>https://hongyuntw.github.io/posts/macbert/</guid>
      <description>&lt;h2 id=&#34;簡介&#34;&gt;簡介&lt;/h2&gt;&#xA;&lt;p&gt;這顆Model是朋友跟我說可以用用看，很多中文的比賽都用這個Model我才知道，因此簡單記錄一下&lt;/p&gt;&#xA;&lt;p&gt;這個Model 主要有這些貢獻&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;進行了廣泛的廣播研究，通過詳細分析語言中文預訓練語言模型在各種任務上的表達。&lt;/li&gt;&#xA;&lt;li&gt;改善傳統BERT欲訓練[MASK] token 不出現在下游任務，導致預訓練任務跟實際任務不符的缺點，macBERT是用同義詞來取代token&lt;/li&gt;&#xA;&lt;li&gt;為了改善中文NLP，發布預訓練Model&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用 WWM (whole word masking) 跟 n-gram 來選要被替換的tokens&#xA;&lt;ul&gt;&#xA;&lt;li&gt;uni-gram - 4-gram 的比例是 40%、30%、20%、10%。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;原始BERT模型使用[MASK] token 進行mask，但是[MASK] token 在fine-tuning / downstream task不會出現，會造成預訓練任務與fintuning的不一致，因此用同義詞來替換token&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A similar word is obtained by using Synonyms toolkit (Wang and Hu, 2017), which is based on word2vec&lt;/li&gt;&#xA;&lt;li&gt;If an N-gram is selected to mask, we will find similar words individually. In rare cases, when there is no similar word, we will degrade to use random word replacement&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;15% input words for masking&#xA;&lt;ul&gt;&#xA;&lt;li&gt;80% will replace with similar words&lt;/li&gt;&#xA;&lt;li&gt;10% replace with a random word&lt;/li&gt;&#xA;&lt;li&gt;keep with original words for the rest of 10%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://hongyuntw.github.io/images/macBERT/1.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RocketQA : An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering</title>
      <link>https://hongyuntw.github.io/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</link>
      <pubDate>Fri, 20 Aug 2021 06:00:20 +0600</pubDate>
      <guid>https://hongyuntw.github.io/posts/rocketqa-an-optimized-training-approach-to-dense-p-024837d049054bc5907bdba7b2e6a105/</guid>
      <description>&lt;h2 id=&#34;abstract--introduction&#34;&gt;Abstract + Introduction&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;difficult to effectively train a dual-encoder for dense passage retrieval&#xA;&lt;ul&gt;&#xA;&lt;li&gt;retriever needs to identify positive passages for each question from a large collection&lt;/li&gt;&#xA;&lt;li&gt;there might be a large number of unlabeled positives (IR資料集可能有錯誤)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;it is expensive to acquire large-scale training data for open-domain QA&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;反正就是large scale 的open-domain QA Model很難train，所以提出了三種方法來改善整個流程&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Discriminative Semantic Ranker for Question Retrieval</title>
      <link>https://hongyuntw.github.io/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</link>
      <pubDate>Sun, 17 Jan 2021 08:06:25 +0600</pubDate>
      <guid>https://hongyuntw.github.io/posts/a-discriminative-semantic-ranker-for-question-retr-e6a82005de084ffd9a7ef19a19120880/</guid>
      <description>&lt;h2 id=&#34;abstract--introduction&#34;&gt;Abstract + Introduction&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;First-stage ranker 的新架構 DenseTrans - Transformer + DenseNet 的融合物&lt;/li&gt;&#xA;&lt;li&gt;主要的任務是question retrieval，找出與使用者輸入相似的問題，類似FAQ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Term-based的方式會有 無法滿足 &lt;strong&gt;Semantic Requirement&lt;/strong&gt; 的缺點&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
